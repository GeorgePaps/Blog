<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Posts on George Paps Blog</title>
        <link>https://georgepaps.pages.dev/post/</link>
        <description>Recent content in Posts on George Paps Blog</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en-us</language>
        <lastBuildDate>Wed, 08 Oct 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://georgepaps.pages.dev/post/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>Glimpses of Computer Vision</title>
        <link>https://georgepaps.pages.dev/post/computervision1/</link>
        <pubDate>Wed, 08 Oct 2025 00:00:00 +0000</pubDate>
        
        <guid>https://georgepaps.pages.dev/post/computervision1/</guid>
        <description>&lt;p&gt;This essay outlines some major developments in the field of computer vision,
beginning with Perceptron and progressing to more recent advances such as EfficientNet.&lt;/p&gt;
&lt;h2 id=&#34;perceptron-1958&#34;&gt;Perceptron (1958)
&lt;/h2&gt;&lt;p&gt;Our journey starts in the late 1950s.
In 1958 Frank Rosenblatt [1], an erudite psychologist,
conceived the earliest and simplest type of artificial neural network -
the &lt;strong&gt;Perceptron&lt;/strong&gt;.
The Perceptron is a &lt;strong&gt;binary classifier&lt;/strong&gt;:
given some input and a set of trainable parameters,
it outputs 0 or 1.
The following equation describes Perceptron&amp;rsquo;s calculations:&lt;/p&gt;
&lt;p&gt;$$
z=\sum_{i=1}^n w_{i}*x_i + b \newline
\text{output:}\
$$
$$
\text{if } z&amp;gt;= 0 \to 1 \newline
\text{if } z &amp;lt; 0 \to 0
$$&lt;/p&gt;
&lt;p&gt;where $w_{i}$ denotes the trainable parameters,
$x_{i}$ the input vector&amp;rsquo;s components,
and $b$ a parameter called bias.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The computations the Perceptron performs are simple,
yet its most important contribution lies in its learning algorithm.&lt;/strong&gt;
The learning algorithm determines
how the Perceptron adjusts its parameters during training.
The learning process is simple,
each training example is presented to Perceptron
and whenever it misclassifies one,
it updates its parameters accordingly.
These adjustments gradually steer the model towards classifying more examples correctly
and reducing errors.&lt;/p&gt;
&lt;p&gt;This simple, example-driven learning process — try, make mistakes, and adjust —
which to some extend is reminiscent of how humans learn,
combined with some biological inspiration behind the idea of Perceptron
gave it a distinct aura and made it very popular.
An important limitation though
is that Perceptron can find a correct solution only on linearly separable training sets.
If the data are not linearly separable (for example the XOR problem),
the perceptron cannot represent a correct solution.
Minsky and Paper famously highlighted this limitation
contributing significantly towards the arrival of the first AI winter in research.&lt;/p&gt;
&lt;h2 id=&#34;lenet-5-1998&#34;&gt;LeNet-5 (1998)
&lt;/h2&gt;&lt;p&gt;It was known that overcoming the limitations of Perceptron
required more complicated network architectures:
networks with many layers of Perceptrons stacked
(known as multilayer Perceptrons - MLP).
An important problem with these architectures
is that the simple training algorithm mentioned for Perceptron
is not applicable&lt;br&gt;
and finding an efficient and scalable algorithm
for training more complicated networks proved to be a rather daunting task.&lt;/p&gt;
&lt;p&gt;In 1989 French machine-learning heavyweight Yann LeCun
introduced LeNet-5 [2].
On a high level LeNet-5 shares many similar characteristics with Perceptron:
both receive inputs,
perform calculations based on trainable parameters,
and classify the output&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;.
LeNet-5 though is a significantly more complicated artificial neural network.
The input has much bigger size
and the calculations are multi-layered and complicated.&lt;/p&gt;
&lt;p&gt;To get a sense of scale,
LeNet-5 has around 60.000 trainable parameters
while a Perceptron usually has a number in the low tens.
This made training the networks very challenging.
And here lies the main contribution of LeNet-5,
LeCun implemented the efficient and still widely used
method for training neural networks called &lt;strong&gt;backpropagation&lt;/strong&gt;.
This development made feasible the training of networks similar to LeNet-5
and contributed to their widespread adoption.&lt;/p&gt;
&lt;p&gt;Aside from the backpropagation training method
LeNet-5 has made other contributions in the field.
It formalized the convolution operation with weight sharing filters,
it demonstrated that successive convolutional layers
can effectively capture spatial hierarchies,
and it also introduced the use of pooling layers.
Additionally, the overall architecture introduced by LeNet-5
was used as the basis for subsequent models.
Overall LeNet-5 is considered one of the foundational models of deep learning.&lt;/p&gt;
&lt;h2 id=&#34;alexnet-2012&#34;&gt;AlexNet (2012)
&lt;/h2&gt;&lt;p&gt;AlexNet, developed by Krizhevsky, Sutskever, and Hinton [3],
is a neural network similar to LeNet-5 but implemented on a significantly larger scale&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;, with important technical innovations and remarkable performance.&lt;/p&gt;
&lt;p&gt;The main contribution of AlexNet is that it
further showcased the feasibility of using deep convolutional neural networks
for real-world computer visions tasks.
It achieved this through a rigorous technical implementation
and through the introduction of many technical innovations as:
(1) ReLU activation functions,
(2) parallel training in 2 GPUs,
(3) dropout regularization and creative data augmentation techniques to avoid overfitting,
and (4) overlapping max-pooling filters.&lt;/p&gt;
&lt;h2 id=&#34;vgg-16-2014&#34;&gt;VGG-16 (2014)
&lt;/h2&gt;&lt;p&gt;VGG-16 introduced by Simonyan and Zisserman in 2015
comprises another important step in the evolution of deep learning models[4].
Compared to AlexNet,
the size of the model increases
from around 60 million to around 138 million trainable parameters,
while the depth of the model increases
from 12 layers to 16 layers.
Another important difference
is that VGG-16 uses the same filter in all layers&lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;,
this simplifies the model architecture,
while some related architectural tricks
help decrease the number of parameters.
Overall though the architecture of the VGG-16 is similar to that of AlexNet.&lt;/p&gt;
&lt;p&gt;The main contribution of VGG-16 consists
in showing that increased model depth(more layers)
is a viable path towards increased model accuracy.
Additionally, its uniform and simplified architecture
was later used as the basis for many deep learning models
and the pre-trained model
was used as the backbone of many computer vision tasks.&lt;/p&gt;
&lt;h2 id=&#34;resnet-2015&#34;&gt;ResNet (2015)
&lt;/h2&gt;&lt;p&gt;VGG-16 demonstrated the effectiveness of training increasingly deeper networks.
However, as the depth of the networks increased,
researchers and practitioners encountered a new challenge:
vanishing and exploding gradients.
This is a problem related to how backpropagation operates.
As the name suggests, backpropagation,
attempts to &amp;ldquo;propagate&amp;rdquo; the error made in the output layer
back through all the involved layers,
estimate each layer&amp;rsquo;s &amp;ldquo;contribution&amp;rdquo; to the error,
and adjust the respective parameters accordingly.
This process for very deep networks is rather challenging
and often results in overestimation or underestimation
of the contribution distant layers have in the error.&lt;/p&gt;
&lt;p&gt;An elegant solution to the problem of vanishing and exploding gradients
was introduced with ResNet[5].
In a typical neural network,
the information propagates through each layer until it reaches the output,
which can cause the contributions of early layers to diminish significantly.
In Resnet, the authors introduced skip connections (or shortcuts),
which allow information from a layer
to bypass some subsequent layers.
These skip connections reduce the effective depth of calculations
and increase the effectiveness of backpropagation in deeper networks.&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion
&lt;/h2&gt;&lt;p&gt;The improvements from one model to the next are mostly incremental
and even VGG-16 bears many similarities to LeNet-5.
Following this development arc of computer vision models
one should naturally mention MobileNet[6] and EfficientNet[7],
which emphasize efficiency and scalability in convolutional networks.
After that however, there is a clear paradigm shift
with the introduction of Vision Transformers (ViTs),
which replace convolution operations with attention mechanisms.&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References
&lt;/h2&gt;&lt;p&gt;[1] F. Rosenblatt, &amp;ldquo;The Perceptron: A Probabilistic Model for Information Storage and Organization in the Brain,&amp;rdquo; &lt;em&gt;Psychological Review&lt;/em&gt;, vol. 65, no. 6, pp. 386–408, 1958. &lt;a class=&#34;link&#34; href=&#34;https://doi.org/10.1037/h0042519&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;DOI: 10.1037/h0042519&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[2] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner, &amp;ldquo;Gradient-based learning applied to document recognition,&amp;rdquo; &lt;em&gt;Proceedings of the IEEE&lt;/em&gt;, vol. 86, no. 11, pp. 2278–2324, 1998. &lt;a class=&#34;link&#34; href=&#34;https://doi.org/10.1109/5.726791&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;DOI: 10.1109/5.726791&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[3] A. Krizhevsky, I. Sutskever, and G. E. Hinton, &amp;ldquo;ImageNet Classification with Deep Convolutional Neural Networks,&amp;rdquo; &lt;em&gt;Advances in Neural Information Processing Systems (NeurIPS)&lt;/em&gt;, 2012. &lt;a class=&#34;link&#34; href=&#34;https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;PDF&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[4] K. Simonyan and A. Zisserman, &amp;ldquo;Very Deep Convolutional Networks for Large-Scale Image Recognition,&amp;rdquo; arXiv preprint arXiv:1409.1556, 2014. &lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1409.1556&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;arXiv:1409.1556&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[5] K. He, X. Zhang, S. Ren, and J. Sun, &amp;ldquo;Deep Residual Learning for Image Recognition,&amp;rdquo; &lt;em&gt;Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)&lt;/em&gt;, 2016. &lt;a class=&#34;link&#34; href=&#34;https://doi.org/10.1109/CVPR.2016.90&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;DOI: 10.1109/CVPR.2016.90&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[6] A. G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang, T. Weyand, M. Andreetto, and H. Adam, &amp;ldquo;MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications,&amp;rdquo; arXiv preprint arXiv:1704.04861, 2017. &lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1704.04861&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;arXiv:1704.04861&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[7] M. Tan and Q. V. Le, &amp;ldquo;EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks,&amp;rdquo; &lt;em&gt;Proceedings of the International Conference on Machine Learning (ICML)&lt;/em&gt;, 2019. &lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1905.11946&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;arXiv:1905.11946&lt;/a&gt;&lt;/p&gt;
&lt;div class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;
&lt;p&gt;LeNet-5 is used to classify handwritten digits between 0 and 9.&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;
&lt;p&gt;The jump in the scale of the model is impressive.
The input size increases from 32x32 black and white images to
227x227 color images,
thus from 1024 data points per example to 154.587 data points per example.
The number of trainable parameters climbs from 60 thousand to 60 million.
The number of layers increases from 7 to 12
and the network classifies the images between a 1000 categories instead of 10.&amp;#160;&lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:3&#34;&gt;
&lt;p&gt;AlexNet uses convolutional filters of varying size and stride, their size for example ranges from 3x3 to 11x11. VGG-16 uses only 3x3 convolutional filters with same padding and stride of 1. It often uses many of those in a row if necessary. The latter trick reduces the number of necessary model parameters.&amp;#160;&lt;a href=&#34;#fnref:3&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
        </item>
        <item>
        <title>Books read 2025</title>
        <link>https://georgepaps.pages.dev/post/books2025/</link>
        <pubDate>Mon, 08 Sep 2025 00:00:00 +0000</pubDate>
        
        <guid>https://georgepaps.pages.dev/post/books2025/</guid>
        <description>&lt;p&gt;To keep up with my reading habit,
I pledged to read 20 books in 2025.
Here follows the list of books I&amp;rsquo;ve read thus far,
the ones with bold are my favorites.
Hope you&amp;rsquo;ll find some inspiration.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;The Tartar Steppe&lt;/strong&gt; by Dino Buzzati - 8/10&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Why Machine Learn: The Elegant Math Behind Modern AI&lt;/strong&gt;  by Anil Ananthaswamy - 9/10&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Mastery&lt;/strong&gt; by &amp;ldquo;Robert Greene&amp;rdquo; - 9/10&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;What I learned Losing a Million Dollars by &amp;ldquo;Jim Paul&amp;rdquo; 6/10&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The Vegetarian by &amp;ldquo;Han Kang&amp;rdquo; - 7/10&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Maxims by &amp;ldquo;Francois de la Rochefoucauld&amp;rdquo; - 8/10&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;An Inquiry into the Nature and Causes of the Wealth of Nations by &amp;ldquo;Adam Smith&amp;rdquo; - 7/10&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Tomorrow, and Tomorrow, and Tomorrow&lt;/strong&gt; by &lt;em&gt;Gabrielle Zevin&lt;/em&gt; - 8/10&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;One Up on Wall Street by &lt;em&gt;Peter Lynch&lt;/em&gt; - 6/10&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Wool Omnibus by &lt;em&gt;Howey Hugh&lt;/em&gt; -  - 6/10 &lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The Magic Circle by &lt;em&gt;Katherine Neville&lt;/em&gt; - 7/10&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The Eight by &lt;em&gt;Katherine Neville&lt;/em&gt; - 7/10&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&amp;ldquo;Sapiens: A Brief History of Humankind&amp;rdquo;&lt;/strong&gt; by Yuval Noah Harari - 10/10&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Police by &lt;em&gt;Jo Nesbo&lt;/em&gt; - 6/10&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;
&lt;p&gt;Apple TV series SILO is based on this book. I liked the series more.&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
        </item>
        <item>
        <title>Books read 2024</title>
        <link>https://georgepaps.pages.dev/post/books2024/</link>
        <pubDate>Tue, 24 Dec 2024 00:00:00 +0000</pubDate>
        
        <guid>https://georgepaps.pages.dev/post/books2024/</guid>
        <description>&lt;p&gt;In case you need some reading inspiration,
here is a list of the books I read in 2024.
I have highlighted my favorite ones.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;&amp;ldquo;On Writing: A Memoir of the Craft&amp;rdquo;&lt;/strong&gt; by Stephen King - 8/10&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&amp;ldquo;Outlive: The Science and Art of Longevity&amp;rdquo;&lt;/strong&gt; by Peter Attia - 8/10&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&amp;ldquo;Slow Productivity: The Lost Art of Accomplishment Without Burnout&amp;rdquo;&lt;/strong&gt; by Cal Newport - 8/10&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&amp;ldquo;The Gene: An Intimate History&amp;rdquo;&lt;/strong&gt; by Siddhartha Mukherjee - 9/10&lt;/li&gt;
&lt;li&gt;The Man who Solved the Markets: How Jim Simons Launched the Quant Revolution by &lt;em&gt;Gregory Zuckerman&lt;/em&gt; - 7/10&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&amp;ldquo;Same as Ever: A Guide to What Never Changes&amp;rdquo;&lt;/strong&gt; by Morgan Housel - 9/10&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&amp;ldquo;The Psychology of Money&amp;rdquo;&lt;/strong&gt; by Morgan Housel - 8/10&lt;/li&gt;
&lt;li&gt;Algebra of Wealth: A Simple Formula for Financial Security by Scott Galloway - 6/10&lt;/li&gt;
&lt;li&gt;The Book of Laughter and Forgetting by &lt;em&gt;Milan Kundera&lt;/em&gt; - 6/10&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&amp;ldquo;Reality is Not What it Seems: The Journey to Quantum Gravity&amp;rdquo;&lt;/strong&gt; by Carlo Rovelli - 8/10&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&amp;ldquo;Absolutely on Music: Conversations with Seiji Ozawa&amp;rdquo;&lt;/strong&gt; by Haruki Murakami - 8/10&lt;/li&gt;
&lt;li&gt;Ways of Seeing by &lt;em&gt;John Berger&lt;/em&gt; - 6/10&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&amp;ldquo;Total Recall: My Unbelievably True Life Story&amp;rdquo;&lt;/strong&gt; by Arnold Schwarzenegger - 8/10&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&amp;ldquo;The Creative Act: A Way of Being&amp;rdquo;&lt;/strong&gt; by Rick Rubin - 9/10&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&amp;ldquo;Atomic Habits&amp;rdquo;&lt;/strong&gt; by James Clear - 8/10&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&amp;ldquo;Heart of Darkness&amp;rdquo;&lt;/strong&gt; by Joseph Conrad - 9/10&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        <item>
        <title>Books read 2023</title>
        <link>https://georgepaps.pages.dev/post/books2023/</link>
        <pubDate>Sat, 23 Dec 2023 00:00:00 +0000</pubDate>
        
        <guid>https://georgepaps.pages.dev/post/books2023/</guid>
        <description>&lt;p&gt;In case you need some reading inspiration,
here is a list of the books I&amp;rsquo;ve read in 2023.
I have highlighted my favorite ones.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;American Pastoral&lt;/em&gt; by Philip Roth - 6/10&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Les Miserables&lt;/em&gt; by Victor Hugo - 7/10&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Clear Thinking&lt;/em&gt; by Shane Parrish - 6/10&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;The Comfort Crisis by Michael Easter&lt;/strong&gt; - 8/10&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Xenakis&lt;/em&gt; by Nouritza Matossian - 7/10&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Benjamin Franklin by Walter Isaacson&lt;/strong&gt; - 8/10&lt;/li&gt;
&lt;li&gt;&lt;em&gt;100 Baggers: Stocks That Return 100-to-1 and How To Find Them&lt;/em&gt; by Christopher W. Mayer - 6/10&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Leonardo Da Vinci by Walter Isaacson&lt;/strong&gt; - 8/10&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Bobby Fischer Teaches Chess&lt;/em&gt; by Bobby Fischer, Stuart Margulies, Don Mosenfelder - 7/10&lt;/li&gt;
&lt;li&gt;&lt;em&gt;1Q84 (All 3 parts)&lt;/em&gt; by Haruki Murakami - 7/10&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Flowers for Algernon&lt;/em&gt; by Daniel Keyes - 7/10&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Gödel, Escher, Bach&lt;/em&gt; by Douglas R. Hofstadter - 6/10&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Steve Jobs by Walter Isaacson&lt;/strong&gt; - 9/10&lt;/li&gt;
&lt;li&gt;&lt;em&gt;The music of the primes&lt;/em&gt; by Marcus du Sautoy - 7/10&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        <item>
        <title>Favorite Books</title>
        <link>https://georgepaps.pages.dev/post/favbooks/</link>
        <pubDate>Mon, 25 Sep 2023 00:00:00 +0000</pubDate>
        
        <guid>https://georgepaps.pages.dev/post/favbooks/</guid>
        <description>&lt;p&gt;The past years I&amp;rsquo;ve read a couple of books.
The ones I enjoyed the most are:&lt;/p&gt;
&lt;p&gt;Non-fiction:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Black Swan - Nassim Nicholas Taleb&lt;/li&gt;
&lt;li&gt;Antifragile - Nassim Nicholas Taleb&lt;/li&gt;
&lt;li&gt;The Bed Of Procrustes - Nassim Nicholas Taleb&lt;/li&gt;
&lt;li&gt;Hackers and Painters - Paul Graham&lt;/li&gt;
&lt;li&gt;Flow - Mihaly Csikszentmihalyi&lt;/li&gt;
&lt;li&gt;Leonardo da Vinci - Walter Isaacson&lt;/li&gt;
&lt;li&gt;Einstein: His Life and Universe - Walter Isaacson&lt;/li&gt;
&lt;li&gt;Steve Jobs - Walter Isaacson&lt;/li&gt;
&lt;li&gt;Thinking, Fast and Slow - Daniel Kahneman&lt;/li&gt;
&lt;li&gt;Benjamin Franklin: An American Life - Walter Isaacson&lt;/li&gt;
&lt;li&gt;Why We Do What We Do - Edward Deci&lt;/li&gt;
&lt;li&gt;The Obstacle Is the Way - Ryan Holiday&lt;/li&gt;
&lt;li&gt;Stillness is the Key - Ryan Holiday&lt;/li&gt;
&lt;li&gt;Ego is the Enemy - Ryan Holiday&lt;/li&gt;
&lt;li&gt;Mastery - Robert Greene&lt;/li&gt;
&lt;li&gt;The Comfort Crisis - Michael Easter&lt;/li&gt;
&lt;li&gt;Why Nations Fail - Daron Acemoglu&lt;/li&gt;
&lt;li&gt;Macroeconomics Beyond the NAIRU - Servaas Storm&lt;/li&gt;
&lt;li&gt;21 Lessons for the 21st Century - Yuval Noah Harari&lt;/li&gt;
&lt;li&gt;Sapiens: A Brief History of Humankind - Yuval Noah Harari&lt;/li&gt;
&lt;li&gt;Essentialism - Greg McKeown&lt;/li&gt;
&lt;li&gt;The Art of the Good Life - Rolf Dobelli&lt;/li&gt;
&lt;li&gt;The Pathless Path - Paul Millerd&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Fiction:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The Count of Monte Cristo - Alexandre Dumas&lt;/li&gt;
&lt;li&gt;One Hundred Years of Solitude - Gabriel Garcia Marquez&lt;/li&gt;
&lt;li&gt;Norwegian Wood - Haruki Murakami&lt;/li&gt;
&lt;li&gt;Laughable Loves - Milan Kundera&lt;/li&gt;
&lt;li&gt;Le Club des Incorrigibles Optimistes - Jean-Michel Guenassia&lt;/li&gt;
&lt;li&gt;The Unbearable Lightness of Being - Milan Kundera&lt;/li&gt;
&lt;li&gt;Love in the Time of Cholera - Gabriel Garcia Marquez&lt;/li&gt;
&lt;li&gt;4 3 2 1 - Paul Auster&lt;/li&gt;
&lt;li&gt;Silk - Alessandro Baricco&lt;/li&gt;
&lt;li&gt;The Stranger - Albert Camus&lt;/li&gt;
&lt;li&gt;When Nietzsche Wept - Irvin Yalom&lt;/li&gt;
&lt;/ol&gt;
</description>
        </item>
        
    </channel>
</rss>
