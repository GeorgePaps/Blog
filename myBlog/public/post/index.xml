<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Posts on George Paps Blog</title>
        <link>https://georgepaps.pages.dev/post/</link>
        <description>Recent content in Posts on George Paps Blog</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en-us</language>
        <lastBuildDate>Sat, 27 Sep 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://georgepaps.pages.dev/post/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>Glimpses of Computer Vision (Draft)</title>
        <link>https://georgepaps.pages.dev/post/computervision1/</link>
        <pubDate>Sat, 27 Sep 2025 00:00:00 +0000</pubDate>
        
        <guid>https://georgepaps.pages.dev/post/computervision1/</guid>
        <description>&lt;p&gt;This essay outlines some major developments in the field of computer vision,
beginning with Perceptron and progressing to more recent advances such as EfficientNet.&lt;/p&gt;
&lt;h2 id=&#34;perceptron-1957&#34;&gt;Perceptron (1957)
&lt;/h2&gt;&lt;p&gt;Our journey will start in the late 50s.
In 1957 Rosenblatt, an erudite psychologist,
conceived the earliest and simplest type of artificial neural network, the Perceptron.
The Perceptron functions as a simple classifier
which based on some inputs outputs 0 or 1.
Its calculations are straightforward:
take the inputs,
calculate their weighted average,
add a parameter called bias,
and check the sign of the result.
If the result is greater or equal to zero the neuron outputs 1,
otherwise if the result is less than 0 it outputs 0.&lt;/p&gt;
&lt;p&gt;In mathematical notation the equation described above for four inputs is:&lt;/p&gt;
&lt;p&gt;$$
z = w_{1}*x_{1} + w_{2}*x_{2} + w_{3}*x_{3} + w_{4}*x_{4} + b \
$$
$$
\text{output:}\
$$
$$
\text{if } z&amp;gt;= 0 \to 1 \newline
\text{if } z &amp;lt; 0 \to 0
$$
or in condensed form $z$ can be calculated as:&lt;/p&gt;
&lt;p&gt;$$z=\sum_{i=1}^n w_{i}*x_i + b $$&lt;/p&gt;
&lt;p&gt;The calculations Perceptron performs seem and are simple.
&lt;strong&gt;They key contribution of Perceptron though,
lies not in its calculations,
but on its learning algorithm.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;To get a better understanding of how Perceptron works,
let&amp;rsquo;s start with a simple house classification problem.
We want, given the square meters, the number of rooms, the construction year,
the price, and an indicator of how expensive the area is,
to identify whether a house is a bargain or not.
Since Perceptron is a classifier
we can use it to help us identify bargains.
The problem statement mentioned above can be restated in the following way:
$$
z = w_{1}*x_{1} + w_{2}*x_{2} + w_{3}*x_{3} + w_{4}*x_{4} + w_{4}*x_{4} + b \
$$&lt;/p&gt;
&lt;p&gt;$$
\text{where:}
$$&lt;/p&gt;
&lt;p&gt;$$
x_{1} \text{: square meters of the house} \newline
x_{2} \text{: number of rooms} \newline
x_{3} \text{: construction year} \newline
x_{4} \text{: price} \newline
x_{5} \text{: area indicator} \newline
$$&lt;/p&gt;
&lt;p&gt;Now, if $z &amp;gt; 0$ we&amp;rsquo;ll say that the house is a bargain, otherwise it is not.&lt;/p&gt;
&lt;p&gt;In the problem statement there are five $x$ variables describing each house,
and  6 parameters $w_{1},w_{2},w_{3},w_{4},w_{5}, w_{6}$, and $b$
that affect the outcome.
These parameters,
when selected appropriately,
can result in the equation correctly identifying
whether a house is a bargain or not.
For the houses that are bargains
their weighted average plus b would be greater than zero
for the rest it will be less than zero.
But how do we decide what values these 6 parameters will take?&lt;/p&gt;
&lt;p&gt;The process of picking the appropriate parameters
for our Perceptron is called training.
For the training process to take place
we need training examples,
houses with known values for the 5 variables describing them
and knowledge of whether they are a bargain or not.
Having these training examples we can tune the parameters
in order to classify a house as a bargain or not.
Without getting into many details,
here is the high level training algorithm used by Perceptron.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;1) pick random values for the parameters w1,w2,w3,w4,w5,b

2) pick an example house:

    a) calculate z based on house variables and parameters
    b) if z &amp;gt; 0 the algorithm predicts the house is a bargain, otherwise not
    c) does the prediction agree with the example?
        i) yes -&amp;gt; continue to next example
        ii) no -&amp;gt;  update the parameters and continue to next example
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The second loop continues until we go through all examples being correctly classified
or some other stopping condition if it takes too many iteration to classify all houses correctly.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;One of the main contributions of Perceptron
is the training algorithm described in step 2.&lt;/strong&gt;
This step-by-step learning by example process captured the imagination
of the scientist of the time.
It might felt reminiscent on some level to how we learn;
we make attempts towards our goal
and when we make a mistake we learn from it
and adjust our strategy.&lt;/p&gt;
&lt;h2 id=&#34;le-net-1998&#34;&gt;Le Net (1998)
&lt;/h2&gt;&lt;p&gt;French machine-learning heavyweight Yann LeCun introduced in 1998 LeNet5,
a convolutional neural network used for image classification&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;.
LeNet5 was pivotal and contributed many innovations in the field
influencing significantly the field of deep learning.&lt;/p&gt;
&lt;p&gt;To those with knowledge of the early stages of deep learning,
LeNet5&amp;rsquo;s architecture should feel familiar.&lt;/p&gt;
&lt;h2 id=&#34;alexnet-2012&#34;&gt;AlexNet (2012)
&lt;/h2&gt;&lt;p&gt;&amp;hellip;&lt;/p&gt;
&lt;h2 id=&#34;vgg-16-2015&#34;&gt;VGG-16 (2015)
&lt;/h2&gt;&lt;p&gt;&amp;hellip;&lt;/p&gt;
&lt;h2 id=&#34;resnet-2015&#34;&gt;ResNet (2015)
&lt;/h2&gt;&lt;p&gt;&amp;hellip;&lt;/p&gt;
&lt;h2 id=&#34;mobilenet-2017&#34;&gt;MobileNet (2017)
&lt;/h2&gt;&lt;p&gt;&amp;hellip;&lt;/p&gt;
&lt;h2 id=&#34;efficient-net-2019&#34;&gt;Efficient Net (2019)
&lt;/h2&gt;&lt;div class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;
&lt;p&gt;LeNet5 takes 32*32 images and classifies them in categories.
The original application was hand-written digit recognition.&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
        </item>
        <item>
        <title>Books read 2025</title>
        <link>https://georgepaps.pages.dev/post/books2025/</link>
        <pubDate>Mon, 08 Sep 2025 00:00:00 +0000</pubDate>
        
        <guid>https://georgepaps.pages.dev/post/books2025/</guid>
        <description>&lt;p&gt;To keep up with my reading habit,
I pledged to read 20 books in 2025.
Here follows the list of books I&amp;rsquo;ve read thus far,
the ones with bold are my favorites.
Hope you&amp;rsquo;ll find some inspiration.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;The Tartar Steppe&lt;/strong&gt; by Dino Buzzati - 8/10&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Why Machine Learn: The Elegant Math Behind Modern AI&lt;/strong&gt;  by Anil Ananthaswamy - 9/10&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Mastery&lt;/strong&gt; by &amp;ldquo;Robert Greene&amp;rdquo; - 9/10&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;What I learned Losing a Million Dollars by &amp;ldquo;Jim Paul&amp;rdquo; 6/10&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The Vegetarian by &amp;ldquo;Han Kang&amp;rdquo; - 7/10&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Maxims by &amp;ldquo;Francois de la Rochefoucauld&amp;rdquo; - 8/10&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;An Inquiry into the Nature and Causes of the Wealth of Nations by &amp;ldquo;Adam Smith&amp;rdquo; - 7/10&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Tomorrow, and Tomorrow, and Tomorrow&lt;/strong&gt; by &lt;em&gt;Gabrielle Zevin&lt;/em&gt; - 8/10&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;One Up on Wall Street by &lt;em&gt;Peter Lynch&lt;/em&gt; - 6/10&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Wool Omnibus by &lt;em&gt;Howey Hugh&lt;/em&gt; -  - 6/10 &lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The Magic Circle by &lt;em&gt;Katherine Neville&lt;/em&gt; - 7/10&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The Eight by &lt;em&gt;Katherine Neville&lt;/em&gt; - 7/10&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&amp;ldquo;Sapiens: A Brief History of Humankind&amp;rdquo;&lt;/strong&gt; by Yuval Noah Harari - 10/10&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Police by &lt;em&gt;Jo Nesbo&lt;/em&gt; - 6/10&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;
&lt;p&gt;Apple TV series SILO is based on this book. I liked the series more.&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
        </item>
        <item>
        <title>Books read 2024</title>
        <link>https://georgepaps.pages.dev/post/books2024/</link>
        <pubDate>Tue, 24 Dec 2024 00:00:00 +0000</pubDate>
        
        <guid>https://georgepaps.pages.dev/post/books2024/</guid>
        <description>&lt;p&gt;In case you need some reading inspiration,
here is a list of the books I read in 2024.
I have highlighted my favorite ones.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;&amp;ldquo;On Writing: A Memoir of the Craft&amp;rdquo;&lt;/strong&gt; by Stephen King - 8/10&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&amp;ldquo;Outlive: The Science and Art of Longevity&amp;rdquo;&lt;/strong&gt; by Peter Attia - 8/10&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&amp;ldquo;Slow Productivity: The Lost Art of Accomplishment Without Burnout&amp;rdquo;&lt;/strong&gt; by Cal Newport - 8/10&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&amp;ldquo;The Gene: An Intimate History&amp;rdquo;&lt;/strong&gt; by Siddhartha Mukherjee - 9/10&lt;/li&gt;
&lt;li&gt;The Man who Solved the Markets: How Jim Simons Launched the Quant Revolution by &lt;em&gt;Gregory Zuckerman&lt;/em&gt; - 7/10&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&amp;ldquo;Same as Ever: A Guide to What Never Changes&amp;rdquo;&lt;/strong&gt; by Morgan Housel - 9/10&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&amp;ldquo;The Psychology of Money&amp;rdquo;&lt;/strong&gt; by Morgan Housel - 8/10&lt;/li&gt;
&lt;li&gt;Algebra of Wealth: A Simple Formula for Financial Security by Scott Galloway - 6/10&lt;/li&gt;
&lt;li&gt;The Book of Laughter and Forgetting by &lt;em&gt;Milan Kundera&lt;/em&gt; - 6/10&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&amp;ldquo;Reality is Not What it Seems: The Journey to Quantum Gravity&amp;rdquo;&lt;/strong&gt; by Carlo Rovelli - 8/10&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&amp;ldquo;Absolutely on Music: Conversations with Seiji Ozawa&amp;rdquo;&lt;/strong&gt; by Haruki Murakami - 8/10&lt;/li&gt;
&lt;li&gt;Ways of Seeing by &lt;em&gt;John Berger&lt;/em&gt; - 6/10&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&amp;ldquo;Total Recall: My Unbelievably True Life Story&amp;rdquo;&lt;/strong&gt; by Arnold Schwarzenegger - 8/10&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&amp;ldquo;The Creative Act: A Way of Being&amp;rdquo;&lt;/strong&gt; by Rick Rubin - 9/10&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&amp;ldquo;Atomic Habits&amp;rdquo;&lt;/strong&gt; by James Clear - 8/10&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&amp;ldquo;Heart of Darkness&amp;rdquo;&lt;/strong&gt; by Joseph Conrad - 9/10&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        <item>
        <title>Books read 2023</title>
        <link>https://georgepaps.pages.dev/post/books2023/</link>
        <pubDate>Sat, 23 Dec 2023 00:00:00 +0000</pubDate>
        
        <guid>https://georgepaps.pages.dev/post/books2023/</guid>
        <description>&lt;p&gt;In case you need some reading inspiration,
here is a list of the books I&amp;rsquo;ve read in 2023.
I have highlighted my favorite ones.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;American Pastoral&lt;/em&gt; by Philip Roth - 6/10&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Les Miserables&lt;/em&gt; by Victor Hugo - 7/10&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Clear Thinking&lt;/em&gt; by Shane Parrish - 6/10&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;The Comfort Crisis by Michael Easter&lt;/strong&gt; - 8/10&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Xenakis&lt;/em&gt; by Nouritza Matossian - 7/10&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Benjamin Franklin by Walter Isaacson&lt;/strong&gt; - 8/10&lt;/li&gt;
&lt;li&gt;&lt;em&gt;100 Baggers: Stocks That Return 100-to-1 and How To Find Them&lt;/em&gt; by Christopher W. Mayer - 6/10&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Leonardo Da Vinci by Walter Isaacson&lt;/strong&gt; - 8/10&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Bobby Fischer Teaches Chess&lt;/em&gt; by Bobby Fischer, Stuart Margulies, Don Mosenfelder - 7/10&lt;/li&gt;
&lt;li&gt;&lt;em&gt;1Q84 (All 3 parts)&lt;/em&gt; by Haruki Murakami - 7/10&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Flowers for Algernon&lt;/em&gt; by Daniel Keyes - 7/10&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Gödel, Escher, Bach&lt;/em&gt; by Douglas R. Hofstadter - 6/10&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Steve Jobs by Walter Isaacson&lt;/strong&gt; - 9/10&lt;/li&gt;
&lt;li&gt;&lt;em&gt;The music of the primes&lt;/em&gt; by Marcus du Sautoy - 7/10&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        <item>
        <title>Favorite Books</title>
        <link>https://georgepaps.pages.dev/post/favbooks/</link>
        <pubDate>Mon, 25 Sep 2023 00:00:00 +0000</pubDate>
        
        <guid>https://georgepaps.pages.dev/post/favbooks/</guid>
        <description>&lt;p&gt;The past years I&amp;rsquo;ve read a couple of books.
The ones I enjoyed the most are:&lt;/p&gt;
&lt;p&gt;Non-fiction:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Black Swan - Nassim Nicholas Taleb&lt;/li&gt;
&lt;li&gt;Antifragile - Nassim Nicholas Taleb&lt;/li&gt;
&lt;li&gt;The Bed Of Procrustes - Nassim Nicholas Taleb&lt;/li&gt;
&lt;li&gt;Hackers and Painters - Paul Graham&lt;/li&gt;
&lt;li&gt;Flow - Mihaly Csikszentmihalyi&lt;/li&gt;
&lt;li&gt;Leonardo da Vinci - Walter Isaacson&lt;/li&gt;
&lt;li&gt;Einstein: His Life and Universe - Walter Isaacson&lt;/li&gt;
&lt;li&gt;Steve Jobs - Walter Isaacson&lt;/li&gt;
&lt;li&gt;Thinking, Fast and Slow - Daniel Kahneman&lt;/li&gt;
&lt;li&gt;Benjamin Franklin: An American Life - Walter Isaacson&lt;/li&gt;
&lt;li&gt;Why We Do What We Do - Edward Deci&lt;/li&gt;
&lt;li&gt;The Obstacle Is the Way - Ryan Holiday&lt;/li&gt;
&lt;li&gt;Stillness is the Key - Ryan Holiday&lt;/li&gt;
&lt;li&gt;Ego is the Enemy - Ryan Holiday&lt;/li&gt;
&lt;li&gt;Mastery - Robert Greene&lt;/li&gt;
&lt;li&gt;The Comfort Crisis - Michael Easter&lt;/li&gt;
&lt;li&gt;Why Nations Fail - Daron Acemoglu&lt;/li&gt;
&lt;li&gt;Macroeconomics Beyond the NAIRU - Servaas Storm&lt;/li&gt;
&lt;li&gt;21 Lessons for the 21st Century - Yuval Noah Harari&lt;/li&gt;
&lt;li&gt;Sapiens: A Brief History of Humankind - Yuval Noah Harari&lt;/li&gt;
&lt;li&gt;Essentialism - Greg McKeown&lt;/li&gt;
&lt;li&gt;The Art of the Good Life - Rolf Dobelli&lt;/li&gt;
&lt;li&gt;The Pathless Path - Paul Millerd&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Fiction:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The Count of Monte Cristo - Alexandre Dumas&lt;/li&gt;
&lt;li&gt;One Hundred Years of Solitude - Gabriel Garcia Marquez&lt;/li&gt;
&lt;li&gt;Norwegian Wood - Haruki Murakami&lt;/li&gt;
&lt;li&gt;Laughable Loves - Milan Kundera&lt;/li&gt;
&lt;li&gt;Le Club des Incorrigibles Optimistes - Jean-Michel Guenassia&lt;/li&gt;
&lt;li&gt;The Unbearable Lightness of Being - Milan Kundera&lt;/li&gt;
&lt;li&gt;Love in the Time of Cholera - Gabriel Garcia Marquez&lt;/li&gt;
&lt;li&gt;4 3 2 1 - Paul Auster&lt;/li&gt;
&lt;li&gt;Silk - Alessandro Baricco&lt;/li&gt;
&lt;li&gt;The Stranger - Albert Camus&lt;/li&gt;
&lt;li&gt;When Nietzsche Wept - Irvin Yalom&lt;/li&gt;
&lt;/ol&gt;
</description>
        </item>
        
    </channel>
</rss>
